<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Variational inference is just Bayesian inference | Luigi Acerbi </title> <meta name="author" content="Luigi Acerbi"> <meta name="description" content="an interactive tutorial on variational inference"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/lab-symbol-small.png?faeae0fcee1eb052509188ee09c250f5"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lacerbi.github.io/blog/2024/vi-is-just-inference/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Variational inference is just Bayesian inference",
            "description": "an interactive tutorial on variational inference",
            "published": "November 27, 2024",
            "authors": [
              
              {
                "author": "Luigi Acerbi",
                "authorURL": "https://en.wikipedia.org/wiki/Albert_Einstein",
                "affiliations": [
                  {
                    "name": "University of Helsinki, Helsinki, Finland",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Luigi</span> Acerbi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/contact/">contact </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Variational inference is just Bayesian inference</h1> <p>an interactive tutorial on variational inference</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#variational-inference-on-a-general-target-density">Variational inference on a general target density</a> </div> <div> <a href="#variational-inference-to-approximate-a-target-posterior">Variational inference to approximate a target posterior</a> </div> <div> <a href="#things-to-know">Things to know</a> </div> </nav> </d-contents> <p>The goal of this post is to show you that variational inference is a very natural way of thinking about Bayesian inference and not some shady approximate method.<d-footnote>Unlike what the MCMC mafia wants you to think.</d-footnote> At the end, you will also be able to directly play around with variational inference via an interactive visualization. In fact, you can also just skip the article and go straight to play with the interactive thingie at the bottom, and then come back if you feel like it.</p> <h2 id="what-is-variational-inference">What is variational inference</h2> <p>Let’s start with the textbook definitions. We have a <em>target</em> distribution</p> \[p^\star(\theta) = \frac{\widetilde{p}(\theta)}{\mathcal{Z}},\] <p>which we know up to its normalization constant \(\mathcal{Z}\). At the core, variational inference is a way to approximate \(p^\star(\theta)\) having only the ability to evaluate the <em>unnormalized</em> target \(\widetilde{p}(\theta)\). The target can be continuous or discrete (or mixed), there are no restrictions!</p> <p>If we go on reading a textbook, it will tell us that variational inference “approximates” the target with a (simpler) distribution \(q_\psi(\theta)\) parameterized by \(\psi\).</p> <p>For example, if \(q\) is a multivariate normal, \(\psi\) could be the mean and covariance matrix of the distribution, \(\psi = (\mu, \Sigma)\). Please note that while normal distributions are a common choice in variational inference, they are not the only one – you could choose \(q\) to be <em>any</em> distribution of your choice!</p> <h3 id="why-do-we-need-to-approximate-the-target">Why do we need to approximate the target?</h3> <p>That is a great question. Why can’t we just use the target as is? <em>Because we can’t.</em></p> <p>Yes, we may be able to evaluate \(\widetilde{p}(\theta)\) for any chosen value of \(\theta\), but that alone does not tell us much.<d-footnote>Even knowing the normalization constant might not help that much.</d-footnote> What is the shape of this distribution? What are its moments? Its covariance structure? Does it have multiple modes? What is the expectation of an arbitrary function \(f(\theta)\) under the target? We may not know any of that!</p> <p><em>One way</em> to compute these values might be to get samples from the target… but how do we get those? How do we draw samples from the target if we only know an unnormalized \(\widetilde{p}(\theta)\)?<d-footnote>Yes, one answer is MCMC (Markov Chain Monte Carlo), as surely you know thanks to the MCMC mafia. Point is, there are other answers.</d-footnote></p> <p>In short, we have our largely-unusable target and we would like to replace it with something that is easy to use and compute with for all the quantities we care about. There is an imponderable word for that: we want a distribution which is <em>tractable</em>.</p> <h3 id="making-the-intractable-tractable">Making the intractable tractable</h3> <p>This is the magic of what variational inference does: it takes an intractable target distribution and it gives back a <em>tractable</em> approximation \(q\), belonging to a class of our choice. What tractable exactly means is up for discussion, but at the very least we expect these properties:</p> <ul> <li>\(q\) is normalized</li> <li>We can draw samples from \(q\)</li> <li>We can evaluate the density of \(q\) at any point</li> </ul> <p>There is potentially a whole variety of desiderata for a tractable distribution, and you are encouraged to read Choi et al. (2020)<d-cite key="choi2020probabilistic"></d-cite>.</p> <h2 id="variational-inference-on-a-general-target-density">Variational inference on a general target density</h2> <p>So, how does \(q\) approximate the target? Intuitively, we want \(q\) to be as similar as possible to the <em>normalized</em> target \(p^\star\).</p> <p>So we can take a measure of discrepancy between two distributions, and say that we want that discrepancy to be as small as possible. Traditionally, variational inference chooses the reverse <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="external nofollow noopener" target="_blank">Kullback-Leibler (KL) divergence</a> as its discrepancy function:<d-footnote>For concreteness, we write the following equations as integrals, but more generally they should be written as expectations (where the notation works for discrete, continuous, and mixed distributions).</d-footnote></p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log \frac{q_\psi(\theta)}{p^\star(\theta)} \, d\theta\] <p>This measures how the approximation \(q_\psi(\theta)\) diverges (differs) from the normalized target distribution \(p^\star(\theta)\). It is <em>reverse</em> because we put the approximation \(q\) first (the KL is not symmetric). The <em>direct</em> KL divergence would have the “real” target distribution \(p^\star\) first.</p> <p>So for a given family of approximating distributions \(q_\psi(\theta)\), variational inference chooses the best value of the parameters \(\psi\) that make \(q_\psi\) “as close as possible” to \(p^\star\) by minimizing the KL divergence between \(q_\psi\) and \(p^\star\).</p> <p>Done? Not quite yet.</p> <h2 id="the-evidence-lower-bound-elbo">The Evidence Lower BOund (ELBO)</h2> <p>There is a caveat to the logic above: remember that we only have the unnormalized \(\widetilde{p}\), we do not have \(p^\star\)! However, it turns out that this is no problem at all. First, we present the main results, and we will provide a full derivation after, but only if you are interested.</p> <p>Minimizing the KL divergence between \(q_\psi\) and \(p^\star\) can be achieved by maximizing the so-called ELBO, or Evidence Lower BOund, defined as:</p> \[\text{ELBO}(q_\psi) = \underbrace{\int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta}_{\text{Cross-entropy}} \; \underbrace{- \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta}_{\text{Entropy}}.\] <p>First, note that the ELBO only depends on \(q_\psi\) and \(\widetilde{p}\). The ELBO is indeed a lower bound to the log normalization constant, that is \(\log \mathcal{Z} \ge \text{ELBO}(\psi)\). It is composed of two terms, a cross-entropy term between \(q\) and \(\widetilde{p}\) and the <strong>entropy</strong> of \(q\). The two terms represent opposing forces:</p> <ul> <li>The (negative) <a href="https://en.wikipedia.org/wiki/Cross-entropy" rel="external nofollow noopener" target="_blank">cross-entropy</a> term ensures that \(q\) avoids regions where \(p\) is low, shrinking towards high-density regions (mode-seeking behavior).</li> <li>The <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="external nofollow noopener" target="_blank">entropy</a> term ensures that \(q\) is as spread-out as possible.</li> </ul> <p>In conclusion, in variational inference we want to tweak the parameters \(\psi\) of \(q\) such that that the approximation \(q_\psi\) is as close as possible to \(p^\star\), according to the ELBO and, equivalently, to the KL divergence.</p> <details><summary>Expand to see the full derivation of the ELBO</summary> <p>This is the full derivation of the ELBO, courtesy of <code class="language-plaintext highlighter-rouge">o1-mini</code> and <code class="language-plaintext highlighter-rouge">gpt-4o</code>, with just a bit of human editing.</p> <hr> <h3 id="step-1-define-the-kl-divergence"><strong>Step 1: Define the KL divergence</strong></h3> <p>The reverse Kullback-Leibler (KL) divergence between \(q_\psi(\theta)\) and the normalized target \(p^\star(\theta)\) is:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log \frac{q_\psi(\theta)}{p^\star(\theta)} \, d\theta\] <hr> <h3 id="step-2-express-pstartheta-in-terms-of-widetildeptheta"><strong>Step 2: Express \(p^\star(\theta)\) in terms of \(\widetilde{p}(\theta)\)</strong></h3> <p>The normalized target \(p^\star(\theta)\) is related to the unnormalized target \(\widetilde{p}(\theta)\) through the normalization constant \(\mathcal{Z}\):</p> \[p^\star(\theta) = \frac{\widetilde{p}(\theta)}{\mathcal{Z}}, \quad \text{where} \quad \mathcal{Z} = \int \widetilde{p}(\theta) \, d\theta.\] <p>Substitute this expression for \(p^\star(\theta)\) into the KL divergence:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log \left( q_\psi(\theta) \cdot \frac{\mathcal{Z}}{\widetilde{p}(\theta)} \right) \, d\theta\] <hr> <h3 id="step-3-split-the-logarithm"><strong>Step 3: Split the logarithm</strong></h3> <p>Using the property of logarithms \(\log(ab) = \log(a) + \log(b)\), split the term inside the integral:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \big( \log q_\psi(\theta) + \log \mathcal{Z} - \log \widetilde{p}(\theta) \big) \, d\theta\] <hr> <h3 id="step-4-separate-the-terms"><strong>Step 4: Separate the terms</strong></h3> <p>Distribute \(q_\psi(\theta)\) over the sum:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta + \int q_\psi(\theta) \log \mathcal{Z} \, d\theta - \int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta\] <hr> <h3 id="step-5-simplify-the-second-term"><strong>Step 5: Simplify the second term</strong></h3> <p>Since \(\mathcal{Z}\) is a constant, \(\log \mathcal{Z}\) is also constant and can be factored out of the integral:</p> \[\int q_\psi(\theta) \log \mathcal{Z} \, d\theta = \log \mathcal{Z} \int q_\psi(\theta) \, d\theta\] <p>Because \(q_\psi(\theta)\) is a valid probability distribution, \(\int q_\psi(\theta) \, d\theta = 1\). Therefore:</p> \[\int q_\psi(\theta) \log \mathcal{Z} \, d\theta = \log \mathcal{Z}\] <hr> <h3 id="step-6-substitute-back"><strong>Step 6: Substitute back</strong></h3> <p>Substitute this simplification back into the KL divergence:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta + \log \mathcal{Z} - \int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta\] <hr> <h3 id="step-7-rearrange-terms"><strong>Step 7: Rearrange terms</strong></h3> <p>Rearrange the equation to isolate \(\log \mathcal{Z}\), grouping terms related to \(q_\psi(\theta)\):</p> \[\log \mathcal{Z} = \text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) + \left( \int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta - \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta \right)\] <hr> <h3 id="step-8-define-the-elbo"><strong>Step 8: Define the ELBO</strong></h3> <p>The ELBO is defined as:</p> \[\text{ELBO}(q_\psi) = \int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta - \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta\] <p>Substitute this into the equation for \(\log \mathcal{Z}\):</p> \[\log \mathcal{Z} = \text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) + \text{ELBO}(q_\psi)\] <hr> <h3 id="step-9-rearrange-for-the-elbo"><strong>Step 9: Rearrange for the ELBO</strong></h3> <p>Rearranging to isolate \(\text{ELBO}(q_\psi)\):</p> \[\text{ELBO}(q_\psi) = \log \mathcal{Z} - \text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta))\] <hr> <h3 id="step-10-interpretation"><strong>Step 10: Interpretation</strong></h3> <ul> <li>\(\log \mathcal{Z}\) is a constant with respect to \(q_\psi(\theta)\).</li> <li>To minimize \(\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta))\), we maximize \(\text{ELBO}(q_\psi)\).</li> </ul> <p>Thus, <strong>minimizing the KL divergence is equivalent to maximizing the ELBO</strong>.</p> </details> <h2 id="variational-inference-to-approximate-a-target-posterior">Variational inference to approximate a target posterior</h2> <p>While variational inference can be performed for any generic target density \(\widetilde{p}(\theta)\), the common scenario is that our target density is an unnormalized <em>posterior distribution</em>:</p> \[\widetilde{p}(\theta) = p(\mathcal{D} \mid \theta) p(\theta) \propto \frac{p(\mathcal{D} \mid \theta) p(\theta)}{p(\mathcal{D})}\] <p>where \(p(\mathcal{D} \mid \theta)\) is the <em>likelihood</em>, \(p(\theta)\) is the <em>prior</em>, and \(p(\mathcal{D} \mid \theta) p(\theta) = p(\mathcal{D}, \theta)\) is the joint distribution. The (unknown) normalization constant here is $\mathcal{Z} \equiv p(\mathcal{D})$, also called the <em>model evidence</em> or <em>marginal likelihood</em>. In this typical usage-case scenario for variational inference, the ELBO reads:</p> \[\text{ELBO}(q_\psi) = \mathbb{E}_{q_\psi(\theta)}\left[ \log p(\mathcal{D} \mid \theta) p(\theta) \right] - \mathbb{E}_{q_\psi(\theta)}\left[\log q_\psi(\theta)\right]\] <p>where we simply replaced \(\widetilde{p}\) with the unnormalized posterior.</p> <h2 id="things-to-know">Things to know</h2> <ul> <li>The term \(\mathbb{E}_{q_\psi(\theta)}\left[ \log p(\theta \mid \mathcal{D}) p(\theta) \right]\) in the ELBO is the expected log joint.</li> <li>The term \(-\mathbb{E}_{q_\psi(\theta)}\left[ \log q_\psi(\theta) \right]\) is the <em>entropy</em> of $q_{\psi}(\theta)$, often written as $\mathcal{H}[q]$.</li> <li>Note that the ELBO is a function of \(\psi\). The optimization finds the \(\psi^*\) that maximizes the ELBO (in practice, the value \(\psi^*\) that minimizes the negative ELBO).</li> <li>The ELBO is a lower bound to the log normalization constant of the target density, that is \(\log p(\mathcal{D})\) when the target is the unnormalized posterior.</li> <li>For notational convenience, the dependence of \(q_\psi(\theta)\) on \(\psi\) is often omitted. Also $$ \psi$ $ is an arbitrary notation, you will find other (Greek) letters to denote the variational parameters.</li> </ul> <iframe src="https://lacerbi.github.io/interactive-vi-demo/" width="100%" height="700px" style="border: none;" title="Interactive Variational Inference Demo"> </iframe> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-11-27-vi-is-just-inference.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Luigi Acerbi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: November 27, 2024. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-contact",title:"contact",description:"Additional contact information",section:"Navigation",handler:()=>{window.location.href="/contact/"}},{id:"post-variational-inference-is-just-bayesian-inference",title:"Variational inference is just Bayesian inference",description:"an interactive tutorial on variational inference",section:"Posts",handler:()=>{window.location.href="/blog/2024/vi-is-just-inference/"}},{id:"post-vi-is-just-inference",title:"Vi Is Just Inference",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/vi-is-just-inference/"}},{id:"post-hello-world",title:"Hello world",description:"Just a test post. Maybe one day there will be real posts, but never trust an academic's promise.",section:"Posts",handler:()=>{window.location.href="/blog/2024/hello-world/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6C%75%69%67%69.%61%63%65%72%62%69@%68%65%6C%73%69%6E%6B%69.%66%69","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0001-7471-7336","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=QYBZoGwAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/lacerbi","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/AcerbiLuigi","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/lacerbi.bsky.social","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>