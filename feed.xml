<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://lacerbi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lacerbi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-27T13:29:50+00:00</updated><id>https://lacerbi.github.io/feed.xml</id><title type="html">blank</title><subtitle>Luigi Acerbi&apos;s personal webpage. </subtitle><entry><title type="html">You can just predict the optimum</title><link href="https://lacerbi.github.io/blog/2025/just-predict-the-optimum/" rel="alternate" type="text/html" title="You can just predict the optimum"/><published>2025-06-26T00:00:00+00:00</published><updated>2025-06-26T00:00:00+00:00</updated><id>https://lacerbi.github.io/blog/2025/just-predict-the-optimum</id><content type="html" xml:base="https://lacerbi.github.io/blog/2025/just-predict-the-optimum/"><![CDATA[<details> <summary><b>tl;dr:</b> A 1-minute summary of this post</summary> <br/> <ul> <li><b>The problem.</b> Standard Bayesian optimization (BO) is powerful for finding optima (e.g., the best hyperparameters for a model), but the classic approach can be complex and inflexible. The loop of fitting a model and choosing where to sample next has many moving parts, and incorporating prior knowledge (an "educated guess") is surprisingly hard.</li> <li><b>The idea.</b> Instead of this intricate loop, what if we could just <em>predict</em> the optimum directly? We propose reframing optimization as a large-scale prediction problem. We pre-train a transformer model (called ACE) on millions of synthetic optimization problems for which the solution is known in advance.</li> <li><b>How it works.</b> For a new, unseen function, the model uses its "learned intuition" to directly predict a full probability distribution over the optimum's location and value from just a few initial samples. The key is a novel generative process we developed to create a massive dataset of complex functions with a guaranteed, <em>known</em> global optimum, which is what we need for training.</li> <li><b>Why it's neat.</b> This "amortized" approach potentially makes optimization much faster at runtime. It simplifies the BO loop and makes it easy to incorporate user knowledge (<em>priors</em>) to guide the search more efficiently. This is a unifying paradigm that connects BO to other machine learning problems like simulation-based inference and image tasks.</li> </ul> </details> <p><a href="https://distill.pub/2020/bayesian-optimization/">Bayesian optimization</a> (BO) is one of the pillars of modern machine learning and scientific discovery. It’s a standard tool for finding the best hyperparameters for a model, the ideal material composition, or the most effective drug compound. The textbook picture of BO is an elegant and simple loop: fit a probabilistic surrogate model (usually a <a href="https://distill.pub/2019/visual-exploration-gaussian-processes/">Gaussian Process</a> aka GP) to your observations, then optimize a so-called <em>acquisition function</em> to decide where to sample next, rinse and repeat.</p> <p>While BO can be very fast nowadays and with solid implementations such as <a href="https://botorch.org/">BoTorch</a>, the classic loop can become intricate and sluggish once you move beyond the most basic or “vanilla” settings. There is a whole zoo of options to choose from: many different Gaussian Process kernels and an ever-growing list of acquisition functions (e.g., Expected Improvement, Upper Confidence Bound, Entropy Search, and many more). Moreover, something that seems like it should be simple in a method that has “Bayesian” in the name – for example, including an educated guess (a <em>prior</em>) about the location or value of the optimum – is not at all straightforward to incorporate into the standard GP framework.</p> <p><strong>But what if, instead of all this, we could just… predict the optimum?</strong></p> <p>The core idea I want to discuss in this blog post is this: if we are smart about it, we can reframe the entire task of optimization as a straightforward prediction problem.<d-footnote>With the inevitable caveats, which we will cover later.</d-footnote> Given a few samples from a function, we can train a neural network to directly output a probability distribution over the location \(\mathbf{x}_{\text{opt}}\) and value \(y_{\text{opt}}\) of the global optimum.<d-footnote>In this post, we follow the convention that the goal is to *minimize* the function, so the global optimum is the *global minimum* of the function.</d-footnote> This is one of the key applications of our recent work on the <a href="https://acerbilab.github.io/amortized-conditioning-engine/">Amortized Conditioning Engine</a> (ACE)<d-cite key="chang2025amortized"></d-cite>.</p> <h2 id="the-core-idea-learning-from-imagination">The core idea: learning from imagination</h2> <p>Think about how humans develop expertise<d-cite key="van2023expertise"></d-cite>. If you solve thousands of Sudoku puzzles, you don’t need to laboriously reason through every new puzzle from scratch. You start recognizing patterns. You get an intuition for where the numbers should go. You are, in a sense, <em>predicting</em> the solution based on the current configuration.<d-footnote>More specifically, there are two distinct modules: a heuristic pattern-recognition module, which is what we are talking about now, and then there is a search/planning module, which we will get back to later.</d-footnote></p> <p>We can do the same with machine learning. If we can generate a virtually infinite dataset of problems with <em>known solutions</em>, we can (pre)train a large model – like a transformer, the same architecture that powers modern Large Language Models (LLMs) – to <em>learn the mapping from problem to solution</em>. This is the essence of <em>amortized inference</em> or <em>meta-learning</em>. For a new problem, the model doesn’t need to reason from first principles; it makes a fast, amortized prediction using its learned “intuition”.<d-footnote>These approaches are called "amortized" because the user pays a large upfront cost *once* for training the big network, but then obtains fast predictions at runtime (the small cost of a neural network forward pass). The large training cost is "amortized" over multiple later predictions, which are performed *without retraining*.</d-footnote></p> <p>The main bottleneck for this approach is similar to the problem faced by modern LLMs: finding the <em>training data</em>. Where do we get a limitless dataset of functions with <em>known optima</em>?</p> <p>While there are well-known techniques to generate functions (for example, using our old friends, the GPs), if we are required to optimize them to know their optimum, it looks like we are back to square one. The functions we want to train on are exactly those difficult, pesky functions where finding the optimum is hard in the first place. Generating such <code class="language-plaintext highlighter-rouge">(function, optimum)</code> pairs would be extremely expensive.</p> <p>But it turns out you can do better than this, if you’re willing to get your hands dirty with a bit of generative modeling.</p> <h2 id="how-to-cook-up-a-function-with-a-known-optimum">How to cook up a function with a known optimum</h2> <p>In our ACE paper, we needed to create a massive dataset of functions to train our model. The challenge was ensuring each function was unique, complex, and – most importantly – had a single, known global optimum \((\mathbf{x}_{\text{opt}}, y_{\text{opt}})\) which we could give our network as a target or label for training. Here is the recipe we came up with, which you can think of in four steps.</p> <h4 id="step-1-choose-the-functions-character">Step 1: Choose the function’s “character”</h4> <p>First, we decide what kind of function we want to generate. Is it very smooth and slowly varying? Is it highly oscillatory? We define this “character” by sampling a <a href="https://www.cs.toronto.edu/~duvenaud/cookbook/">kernel for a Gaussian Process</a> (GP), such as an RBF or Matérn kernel, along with its hyperparameters (like length scales). This gives us a prior over a certain “style” of functions.</p> <h4 id="step-2-pick-a-plausible-optimum">Step 2: Pick a plausible optimum</h4> <p>Next, we choose a location for the global optimum, \(\mathbf{x}_{\text{opt}}\), usually by sampling it uniformly within a box. Then comes an interesting trick. We don’t just pick any value \(y_{\text{opt}}\). To make it realistic, we sample it from the <em>minimum-value distribution</em> for the specific GP family we chose in Step 1. This ensures that the optimum’s value is statistically plausible for that function style. With a small probability, we bump the minimum to be even lower, to make our method robust to “unexpectedly low” minima.</p> <h4 id="step-3-ensuring-a-known-global-optimum">Step 3: Ensuring a known global optimum</h4> <p>Then, we generate a function from the GP prior (defined in Step 1) by <em>conditioning</em> it to pass through our chosen optimum location and value, \((\mathbf{x}_{\text{opt}}, y_{\text{opt}})\) established in Step 2. This is done by treating the optimum as a known data point.</p> <p>However, simply forcing the function to go through this point is not enough. The GP is a flexible, random process; a sample from it might wiggle around and create an even lower minimum somewhere else by chance. To train our model, we need to be <em>certain</em> that \((\mathbf{x}_{\text{opt}}, y_{\text{opt}})\) is the true global optimum.</p> <p>To guarantee this, we apply a transformation. As detailed in our paper’s appendix, we modify the function by adding a <em>convex envelope</em>. We transform all function values \(y_{i}\) like this:</p> \[y_{i}^{\prime} = y_{\text{opt}} + |y_{i} - y_{\text{opt}}| + \frac{1}{5}\|\mathbf{x}_{\text{opt}} - \mathbf{x}_{i}\|^{2}.\] <p>Let’s break down what this does. The term \(y_{\text{opt}} + \lvert y_{i} - y_{\text{opt}}\rvert\) is key. If a function value \(y_i\) is already above our chosen optimum \(y_{\text{opt}}\), it remains unchanged. However, if \(y_i\) happens to be <em>below</em> the optimum, this term reflects it upwards, placing it <em>above</em> \(y_{\text{opt}}\). Then, we add the quadratic “bowl” term that has its lowest point exactly at \(\mathbf{x}_{\text{opt}}\).<d-footnote>The \(1/5\) is a magic number that governs the curvature of the bowl; you can change it.</d-footnote> This bowl smoothly lifts every point of the function, but lifts points farther from \(\mathbf{x}_{\text{opt}}\) more than those nearby. The result is a new function that is guaranteed to have its single global minimum right where we want it.<d-footnote>The implementation in the paper is slightly different but mathematically equivalent: we first generate functions with an optimum at zero, apply a similar transformation, and then add a random vertical offset. The formula here expresses the same idea more directly.</d-footnote></p> <p>This is a simple but effective way to ensure the ground truth for our generative process is, in fact, true. Without it, we would be feeding our network noisy labels, where the provided “optimum” isn’t always the real one.</p> <h4 id="step-4-final-touches">Step 4: Final touches</h4> <p>With the function’s shape secured, we simply sample the data points (the \((x, y)\) pairs) that we’ll use for training. We also add a random vertical offset to the whole function. This prevents the model from cheating by learning, for example, that optima are always near \(y=0\).</p> <p>By repeating this recipe millions of times, we can build a massive, diverse dataset of <code class="language-plaintext highlighter-rouge">(function, optimum)</code> pairs. The hard work is done. Now, we just need to learn from it.</p> <figure style="text-align: center;"> <img src="/assets/img/posts/just-predict-the-optimum/generating-functions.png" alt="Examples of one-dimensional functions generated for training ACE." style="width:100%; max-width: 600px; margin-left: auto; margin-right: auto; display: block;"/> <figcaption style="font-style: italic; margin-top: 10px; margin-bottom: 20px;">Examples of one-dimensional functions with a known global optimum (red dot) from our training dataset. We can quickly generate a virtually infinite number of such functions in any dimension, with varying degrees of complexity.</figcaption> </figure> <h2 id="a-transformer-that-predicts-optima">A transformer that predicts optima</h2> <p>Once you have this dataset, the rest is fairly standard machine learning. We feed our model, ACE, a “context set” consisting of a few observed <code class="language-plaintext highlighter-rouge">(x, y)</code> pairs from a function. The model’s task is to predict the latent variables we care about: \(\mathbf{x}_{\text{opt}}\) and \(y_{\text{opt}}\). Here the term <em>latent</em> is taken from the language of probabilistic modeling, and simply means “unknown”, as opposed to the <em>observed</em> function values.</p> <p>Because ACE is a transformer, it uses the attention mechanism to see the relationships between the context points and we set it up to output a full predictive distribution for the optimum, not just a single point estimate. This means we get uncertainty estimates for free, which is crucial for any Bayesian approach.</p> <p>In addition to predicting the latent variables, ACE can also predict data, i.e., function values \(y^\star\) at any target point \(\mathbf{x}^\star\), following the recipe of similar models such as the Transformer Neural Process (TNP)<d-cite key="nguyen2022transformer"></d-cite> and Prior-Fitted Networks (PFNs)<d-cite key="muller2022transformers"></d-cite>. ACE differs from these previous models in that it is the first architecture that allows the user to explicitly condition on and predict latent variables for the task of interest – such as the optimum location and value in BO –, and not just data points.</p> <figure style="text-align: center;"> <img src="/assets/img/posts/just-predict-the-optimum/bo-prediction-conditioning.png" alt="ACE predicting the optimum location and value in Bayesian Optimization." style="width:100%; max-width: 700px; margin-left: auto; margin-right: auto; display: block;"/> <figcaption style="font-style: italic; margin-top: 10px; margin-bottom: 20px;">ACE can directly predict distributions over the optimum's location \( p(x_{\text{opt}}\mid\mathcal{D}) \) and value \( p(y_{\text{opt}}\mid\mathcal{D}) \) (left panel). These predictions can be further refined by conditioning on additional information, for example by providing a known value for the optimum \( y_{\text{opt}} \) (right panel). Note that the predictions are sensible: for example, in the left panel, the prediction of the value of the optimum (orange distribution) is *equal or below* the lowest observed function value. This is not hard-coded, but entirely learnt by our network! Also note that the conditioning on a known \( y_{\text{opt}} \) value in the right panel "pulls down" the function predictions.</figcaption> </figure> <h2 id="the-bo-loop-with-ace">The BO loop with ACE</h2> <p>So we have a model that, given a few observations, can predict a probability distribution over the optimum’s location and value. How do we use this to power the classic Bayesian optimization loop?</p> <p>At each step, we need to decide which point \(\mathbf{x}\_{\text{next}}\) to evaluate. This choice is guided by an <em>acquisition function</em>. One of the most intuitive acquisition strategies is <a href="https://en.wikipedia.org/wiki/Thompson_sampling">Thompson sampling</a>, which suggests that we should sample our next point from our current belief about where the optimum is. For us, this would mean sampling from \(p(\mathbf{x}\_{\text{opt}}\mid\mathcal{D})\), which we can easily do with ACE.</p> <p>But there’s a subtle trap here. If we just sample from our posterior over the optimum’s location, we risk getting stuck. The model’s posterior will naturally concentrate around the best point seen so far – which is a totally sensible belief to hold. However, sampling from it might lead us to repeatedly query points in the same “good” region without ever truly exploring for a <em>great</em> one. The goal is to find a <em>better</em> point, not just to confirm where we think the current optimum is.</p> <p>This is where having predictive distributions over both the optimum’s location <em>and</em> value becomes relevant. With ACE, we can use an enhanced version of Thompson sampling that explicitly encourages exploration (see <d-cite key="dutordoir2023neural"></d-cite> for a similar approach):</p> <ol> <li>First, we “imagine” (aka <em>phantasize</em>) a better outcome. We sample a target value \(y_{\text{opt}}^\star\) from our predictive distribution \(p(y_{\text{opt}}\mid\mathcal{D})\), but with the crucial constraint that this value must be <em>lower</em> than the best value, \(y_{\text{min}}\), observed so far.</li> <li>Then, we ask the model: “Given that we’re aiming for this new, better score, where should we look?” We then sample the next location \(\mathbf{x}_{\text{next}}\) from the conditional distribution \(p(\mathbf{x}_{\text{opt}}\mid\mathcal{D}, y_{\text{opt}}^\star)\).</li> </ol> <p>This two-step process elegantly balances exploitation (by conditioning on data) and exploration (by forcing the model to seek improvement). It’s a simple, probabilistic way to drive the search towards new and better regions of the space, as shown in the example below.</p> <p>While this enhanced Thompson Sampling is powerful and simple, the story doesn’t end here. Since ACE gives us access to these explicit predictive distributions, implementing more sophisticated, information-theoretic acquisition functions (like Max-value Entropy Search or MES<d-cite key="wang2017max"></d-cite>) becomes much more straightforward than in traditional GP-based BO, which requires complex approximations.</p> <figure style="text-align: center;"> <img src="/assets/img/posts/just-predict-the-optimum/bo-evolution.png" alt="Evolution of ACE's predictions during Bayesian optimization." style="width:100%; max-width: 700px; margin-left: auto; margin-right: auto; display: block;"/> <figcaption style="font-style: italic; margin-top: 10px; margin-bottom: 40px;">An example of ACE in action for Bayesian optimization. In each step (from left to right), ACE observes a new point (red asterisk) and updates its beliefs. The orange distribution on the left is the model's prediction for the optimum's *value* (\(y_{\text{opt}}\)). The red distribution at the bottom is the prediction for the optimum's *location* (\(x_{\text{opt}}\)), which gets more certain with each observation.</figcaption> </figure> <h2 id="what-if-you-already-have-a-good-guess">What if you already have a good guess?</h2> <p>Predicting the optimum from a few data points is powerful, but what if you’re not starting from complete ignorance? Often, you have some domain knowledge. For example, if you are tuning the hyperparameters of a neural network, you might have a strong hunch that the optimal learning rate is more likely to be in the range \([0.0001, 0.01]\) than around \(1.0\). This kind of information is called a <em>prior</em> in Bayesian terms.</p> <p>Incorporating priors into the standard Bayesian optimization loop is surprisingly tricky. While the Bayesian framework is all about updating beliefs, shoehorning prior knowledge about the <em>optimum’s location or value</em> into a standard Gaussian Process model is not straightforward and either requires heuristics or complex, custom solutions (see, for example, <d-cite key="hvarfner2022pi"></d-cite><d-cite key="hvarfner2024general"></d-cite>).</p> <p>This is another area where an amortized approach shines. Because we control the training data generation, we can teach ACE not only to predict the optimum but also how to listen to and use a prior. During its training, we don’t just show ACE functions; we also provide it with various “hunches” (priors of different shapes and strengths) about where the optimum might be for those functions, or for its value. By seeing millions of examples, ACE learns to combine the information from the observed data points with the hint provided by the prior.</p> <p>At runtime, the user can provide a prior distribution over the optimum’s location, \(p(\mathbf{x}_{\text{opt}})\), or value \(p(y_{\text{opt}})\), as a simple histogram across each dimension. ACE then seamlessly integrates this information to produce a more informed (and more constrained) prediction for the optimum. This allows for even faster convergence, as the model doesn’t waste time exploring regions that the user already knows are unpromising. Instead of being a complex add-on, incorporating prior knowledge becomes another natural part of the prediction process.</p> <figure style="text-align: center;"> <img src="/assets/img/posts/just-predict-the-optimum/bo-with-prior.png" alt="Comparison of Bayesian optimization with and without an informative prior on the optimum location." style="width:100%; max-width: 700px; margin-left: auto; margin-right: auto; display: block;"/> <figcaption style="font-style: italic; margin-top: 10px; margin-bottom: 20px;">ACE can seamlessly incorporate user-provided priors. Left: Without a prior, the posterior over the optimum location is based only on the observed data. Right: An informative prior (light blue) about the optimum's location focuses the model's posterior belief (blue), demonstrating how domain knowledge can guide the optimization process more efficiently.</figcaption> </figure> <h2 id="conclusion-a-unifying-paradigm">Conclusion: A unifying paradigm</h2> <p>The main takeaway is that by being clever about data generation, we can transform traditionally complex inference and reasoning problems into large-scale prediction tasks. This approach unifies seemingly disparate fields. In the ACE paper, we show that the <em>exact same architecture</em> can be used for Bayesian optimization, simulation-based inference (predicting simulator parameters from data), and even image completion and classification (predicting class labels or missing pixels).</p> <p>Everything – well, <em>almost</em> everything – boils down to conditioning on data and possibly task-relevant latents (or prior information), and predicting data or other task-relevant latent variables, where what the “latent variable” is depends on the task. For example, in BO, as we saw in this blog post, the latents of interest are the location \(\mathbf{x}_{\text{opt}}\) and value \(y_{\text{opt}}\) of the global optimum.</p> <figure style="text-align: center;"> <img src="/assets/img/posts/just-predict-the-optimum/ace-tasks-compact.png" alt="Diagram showing ACE as a unifying paradigm for different ML tasks." style="width:80%; max-width: 700px; margin-left: auto; margin-right: auto; display: block;"/> <figcaption style="font-style: italic; margin-top: 10px; margin-bottom: 20px;">The ACE framework. Many tasks, like image completion and classification, Bayesian optimization, and simulation-based inference, can be framed as problems of probabilistic conditioning and prediction over data and latent variables.</figcaption> </figure> <p>This is not to say that traditional methods are obsolete. They provide the theoretical foundation and are indispensable when you can’t generate realistic training data. But as our simulators get better and our generative models more powerful, the paradigm of “just predicting” the answer is becoming an increasingly powerful and practical alternative – see for example this recent position paper<d-cite key="muller2025position"></d-cite>, which makes similar points. Overall, it’s a simple idea, but it has the potential to change how we approach many hard problems in science and engineering.</p> <details> <summary>Of course, there are some caveats...</summary> <br/> The "just predict it" approach is powerful, but it's not magic -- yet. Here are a few limitations and active research directions to keep in mind: <ul> <li><b>The need for known latents.</b> The entire "learning from imagination" approach hinges on being able to generate a training dataset with known solutions (the latent variables). For Bayesian optimization, we showed a neat way to generate functions with a known optimum. But for other problems, generating this ground-truth data might be difficult, and you may have first to actually solve the problem for each training example.</li> <li><b>The curse of distribution shift.</b> Like any ML model, ACE fails on data that looks nothing like what it was trained on. If you train it on smooth functions and then ask it to optimize something that looks like a wild, jagged mess, its predictions can become unreliable. This "out-of-distribution" problem is a major challenge in ML, and an active area of research.</li> <li><b>Scaling.</b> Since ACE is based on a vanilla transformer, it has a well-known scaling problem: the computational cost grows quadratically with the number of data points you feed it. For a few dozen points, it's fast, but for thousands, it becomes sluggish. Luckily, there are tricks from the LLM literature that can be applied.</li> <li><b>From specialist to generalist.</b> The current version of ACE is a specialist: you train it for one kind of task (like Bayesian optimization). A major next step is to build a true generalist that can learn to solve many different kinds of problems at once.</li> </ul> </details> <h3 id="teaser-from-prediction-only-to-active-search">Teaser: From prediction only to active search</h3> <p>Direct prediction is only one part of the story. As we hinted at earlier, a key component of intelligence – both human and artificial – isn’t just pattern recognition, but also <em>planning</em> or <em>search</em> (the “thinking” part of modern LLMs and large reasoning models). This module actively decides what to do next to gain the most information. The acquisition strategies we covered are a form of planning which is <em>not</em> amortized. Conversely, we have been working on a more powerful and general framework that tightly integrates amortized inference with amortized active data acquisition. This new system is called <a href="https://arxiv.org/abs/2506.07259">Amortized Active Learning and Inference Engine (ALINE)</a><d-cite key="huang2025aline"></d-cite>, where we use reinforcement learning to teach a model not only to predict, but also how to actively <em>seek</em> information in an amortized manner. But that’s a story for another day.</p> <blockquote> <p>The Amortized Conditioning Engine (ACE) is a new, general-purpose framework for multiple kinds of prediction tasks. On the <a href="https://acerbilab.github.io/amortized-conditioning-engine/">paper website</a> you can find links to all relevant material including code, and we are actively working on extending the framework in manifold ways. If you are interested in this line of research, please get in touch!</p> </blockquote> <div class="acknowledgments-box"> <b>Acknowledgments.</b> This post was written with the assistance of Google's Gemini 2.5 Pro using <a href="https://athanor.works">Athanor</a>, the open-source workbench for AI-assisted coding and writing. </div>]]></content><author><name>Luigi Acerbi</name></author><category term="bayesian-optimization"/><category term="amortized-inference"/><category term="meta-learning"/><summary type="html"><![CDATA[instant Bayesian optimization! (kind of)]]></summary></entry><entry><title type="html">Variational inference is Bayesian inference is optimization</title><link href="https://lacerbi.github.io/blog/2024/vi-is-inference-is-optimization/" rel="alternate" type="text/html" title="Variational inference is Bayesian inference is optimization"/><published>2024-12-04T00:00:00+00:00</published><updated>2024-12-04T00:00:00+00:00</updated><id>https://lacerbi.github.io/blog/2024/vi-is-inference-is-optimization</id><content type="html" xml:base="https://lacerbi.github.io/blog/2024/vi-is-inference-is-optimization/"><![CDATA[<p>The goal of this post is to show that variational inference is a natural way of thinking about Bayesian inference and not some shady approximate method.<d-footnote>Unlike what the MCMC mafia wants you to think.</d-footnote> At the end, you will be able to play around with variational inference via an interactive visualization. In fact, you can also just skip the article and go straight to play with the <a href="https://lacerbi.github.io/blog/2024/vi-is-inference-is-optimization/#playing-with-variational-inference">interactive thingie at the bottom</a>, and then come back if you feel like it.</p> <h2 id="what-is-variational-inference">What is variational inference?</h2> <p>Let’s start with the textbook definitions. We have a <em>target</em> distribution</p> \[p^\star(\theta) = \frac{\widetilde{p}(\theta)}{\mathcal{Z}},\] <p>which we know up to its normalization constant \(\mathcal{Z}\). At the core, variational inference is a way to approximate \(p^\star(\theta)\) having only the ability to evaluate the <em>unnormalized</em> target \(\widetilde{p}(\theta)\). The target can be continuous or discrete (or mixed), there are no restrictions!</p> <p>If we go on reading a textbook, it will tell us that variational inference “approximates” the target with a (simpler) distribution \(q_\psi(\theta)\) parameterized by \(\psi\).</p> <p>For example, if \(q\) is a multivariate normal, \(\psi\) could be the mean and covariance matrix of the distribution, \(\psi = (\mu, \Sigma)\). Please note that while normal distributions are a common choice in variational inference, they are not the only one – you could choose \(q\) to be <em>any</em> distribution family of your choice!</p> <h3 id="why-do-we-need-to-approximate-the-target">Why do we need to approximate the target?</h3> <p>That is a great question. Why can’t we just use the target as is? <em>Because we can’t.</em></p> <p>Yes, we may be able to evaluate \(\widetilde{p}(\theta)\) for any chosen value of \(\theta\), but that alone does not tell us much.<d-footnote>Even knowing the normalization constant might not help that much.</d-footnote> What is the shape of this distribution? What are its moments? Its covariance structure? Does it have multiple modes? What is the expectation of an arbitrary function \(f(\theta)\) under the target? What if $\theta$ is a vector and we want to <em>condition on</em> or <em>marginalize out</em> some values of it? We may not know nor be able to do any of that!</p> <p><em>One way</em> to compute some of these values (and not even all of them) might be to get samples from the target… but how do we get those? How do we draw samples from the target if we only know an unnormalized \(\widetilde{p}(\theta)\)?<d-footnote>Yes, one answer is MCMC (Markov Chain Monte Carlo), as surely you know thanks to the MCMC mafia. Point is, there are other answers.</d-footnote></p> <p>In short, we have our largely-unusable target and we would like to replace it with something that is easy to use and compute with for all the quantities we care about. There is an imponderable word for that: we want a distribution which is <em>tractable</em>.</p> <h3 id="making-the-intractable-tractable">Making the intractable tractable</h3> <p>This is the magic of what variational inference does: it takes an intractable target distribution and it gives back a <em>tractable</em> approximation \(q\), belonging to a class of our choice. We are using here tractable in a loose sense, meaning that we expect these minimal properties of a respectable probability distribution:</p> <ul> <li>\(q\) is normalized</li> <li>We can draw samples from \(q\)</li> <li>We can evaluate the density of \(q\) at any point</li> </ul> <p>There are more precise and nuanced definitions of tractability based on the specific type of probabilistic queries we can compute in polynomial time (e.g., marginals, conditionals, expectations, etc.), and you are encouraged to read Choi et al. (2020)<d-cite key="choi2020probabilistic"></d-cite>.</p> <h2 id="variational-inference-on-a-general-target-density">Variational inference on a general target density</h2> <p>So, how does \(q\) approximate the target? Intuitively, we want \(q\) to be as similar as possible to the <em>normalized</em> target \(p^\star\).</p> <p>So we can take a measure of discrepancy between two distributions, and say that we want that discrepancy to be as small as possible. Traditionally, variational inference chooses the reverse <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler (KL) divergence</a> as its discrepancy function:<d-footnote>For concreteness, we write the following equations as integrals, but more generally they should be written as expectations (where the notation works for discrete, continuous, and mixed distributions).</d-footnote></p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log \frac{q_\psi(\theta)}{p^\star(\theta)} \, d\theta\] <p>This measures how the approximation \(q_\psi(\theta)\) diverges (differs) from the normalized target distribution \(p^\star(\theta)\). It is <em>reverse</em> because we put the approximation \(q\) first (the KL is not symmetric). The <em>direct</em> KL divergence would have the “real” target distribution \(p^\star\) first.</p> <p>So for a given family of approximating distributions \(q_\psi(\theta)\), variational inference chooses the best value of the parameters \(\psi\) that make \(q_\psi\) “as close as possible” to \(p^\star\) by minimizing the KL divergence between \(q_\psi\) and \(p^\star\).</p> <p>Done? Not quite yet.</p> <h3 id="the-evidence-lower-bound-elbo">The Evidence Lower BOund (ELBO)</h3> <p>There is a caveat to the logic above: remember that we only have the unnormalized \(\widetilde{p}\), we do not have \(p^\star\)! However, it turns out that this is no problem at all. First, we present the main results, and we will provide a full derivation after, for the interested readers.</p> <p>Minimizing the KL divergence between \(q_\psi\) and \(p^\star\) can be achieved by maximizing the so-called ELBO, or Evidence Lower BOund, defined as:</p> \[\text{ELBO}(q_\psi) = \underbrace{\int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta}_{\text{Negative cross-entropy}} \; \underbrace{- \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta}_{\text{Entropy}}.\] <p>First, note that the ELBO only depends on \(q_\psi\) and \(\widetilde{p}\). The ELBO takes its name because it is indeed a lower bound to the log normalization constant, that is \(\log \mathcal{Z} \ge \text{ELBO}(\psi)\).<d-footnote>The ELBO name will make even more sense as the *evidence* lower bound when we move to the context of Bayesian inference, where the normalization constant is called the *evidence*, as described later in this post.</d-footnote></p> <p>The ELBO is composed of two terms, a cross-entropy term between \(q\) and \(\widetilde{p}\) and the entropy of \(q\). The two terms represent opposing forces:</p> <ul> <li>The negative <a href="https://en.wikipedia.org/wiki/Cross-entropy">cross-entropy</a> term ensures that \(q\) avoids regions where \(p\) is low, shrinking towards high-density regions (mode-seeking behavior).</li> <li>The <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a> term ensures that \(q\) is as spread-out as possible.</li> </ul> <p>In conclusion, in variational inference we want to tweak the parameters \(\psi\) of \(q\) such that that the approximation \(q_\psi\) is as close as possible to \(p^\star\), according to the ELBO and, equivalently, to the KL divergence.</p> <details><summary>Expand to see the full derivation of the ELBO</summary> <p>This is the full derivation of the ELBO, courtesy of <code class="language-plaintext highlighter-rouge">o1-mini</code> and <code class="language-plaintext highlighter-rouge">gpt-4o</code>, with just a sprinkle of human magic.</p> <hr/> <h3 id="step-1-define-the-kl-divergence"><strong>Step 1: Define the KL divergence</strong></h3> <p>The reverse Kullback-Leibler (KL) divergence between \(q_\psi(\theta)\) and the normalized target \(p^\star(\theta)\) is:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log \frac{q_\psi(\theta)}{p^\star(\theta)} \, d\theta\] <hr/> <h3 id="step-2-express-pstartheta-in-terms-of-widetildeptheta"><strong>Step 2: Express \(p^\star(\theta)\) in terms of \(\widetilde{p}(\theta)\)</strong></h3> <p>The normalized target \(p^\star(\theta)\) is related to the unnormalized target \(\widetilde{p}(\theta)\) through the normalization constant \(\mathcal{Z}\):</p> \[p^\star(\theta) = \frac{\widetilde{p}(\theta)}{\mathcal{Z}}, \quad \text{where} \quad \mathcal{Z} = \int \widetilde{p}(\theta) \, d\theta.\] <p>Substitute this expression for \(p^\star(\theta)\) into the KL divergence:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log \left( q_\psi(\theta) \cdot \frac{\mathcal{Z}}{\widetilde{p}(\theta)} \right) \, d\theta\] <hr/> <h3 id="step-3-split-the-logarithm"><strong>Step 3: Split the logarithm</strong></h3> <p>Using the property of logarithms \(\log(ab) = \log(a) + \log(b)\), split the term inside the integral:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \big( \log q_\psi(\theta) + \log \mathcal{Z} - \log \widetilde{p}(\theta) \big) \, d\theta\] <hr/> <h3 id="step-4-separate-the-terms"><strong>Step 4: Separate the terms</strong></h3> <p>Distribute \(q_\psi(\theta)\) over the sum:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta + \int q_\psi(\theta) \log \mathcal{Z} \, d\theta - \int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta\] <hr/> <h3 id="step-5-simplify-the-second-term"><strong>Step 5: Simplify the second term</strong></h3> <p>Since \(\mathcal{Z}\) is a constant, \(\log \mathcal{Z}\) is also constant and can be factored out of the integral:</p> \[\int q_\psi(\theta) \log \mathcal{Z} \, d\theta = \log \mathcal{Z} \int q_\psi(\theta) \, d\theta\] <p>Because \(q_\psi(\theta)\) is a valid, normalized probability distribution, \(\int q_\psi(\theta) \, d\theta = 1\). Therefore:</p> \[\int q_\psi(\theta) \log \mathcal{Z} \, d\theta = \log \mathcal{Z}\] <p>Substitute this simplification back into the KL divergence:</p> \[\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) = \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta + \log \mathcal{Z} - \int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta\] <hr/> <h3 id="step-6-rearrange-terms"><strong>Step 6: Rearrange terms</strong></h3> <p>Rearrange the equation to isolate \(\log \mathcal{Z}\), grouping terms related to \(q_\psi(\theta)\):</p> \[\log \mathcal{Z} = \text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) + \left( \int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta - \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta \right)\] <hr/> <h3 id="step-7-define-the-elbo"><strong>Step 7: Define the ELBO</strong></h3> <p>The ELBO is defined as:</p> \[\text{ELBO}(q_\psi) = \int q_\psi(\theta) \log \widetilde{p}(\theta) \, d\theta - \int q_\psi(\theta) \log q_\psi(\theta) \, d\theta\] <p>Substitute this into the equation for \(\log \mathcal{Z}\):</p> \[\log \mathcal{Z} = \text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta)) + \text{ELBO}(q_\psi)\] <hr/> <h3 id="step-8-rearrange-for-the-elbo"><strong>Step 8: Rearrange for the ELBO</strong></h3> <p>Rearranging to isolate \(\text{ELBO}(q_\psi)\):</p> \[\text{ELBO}(q_\psi) = \log \mathcal{Z} - \text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta))\] <hr/> <h3 id="step-9-interpretation"><strong>Step 9: Interpretation</strong></h3> <ul> <li>\(\log \mathcal{Z}\) is a constant with respect to \(q_\psi(\theta)\).</li> <li>To minimize \(\text{KL}(q_\psi(\theta) \,\mid\mid\, p^\star(\theta))\), we maximize \(\text{ELBO}(q_\psi)\).</li> </ul> <p>Thus, <strong>minimizing the KL divergence is equivalent to maximizing the ELBO</strong>.</p> <p>Moreover, since the $\text{KL}$ divergence is non-negative and zero if $p = q$:</p> <ul> <li>$\text{ELBO}(q_\psi) \le \log \mathcal{Z} \Longrightarrow$ the ELBO is a lower bound to $\log Z$.</li> <li>If $q = p$, $\text{ELBO}(q_\psi) = \log \mathcal{Z}$.</li> </ul> </details> <details><summary>Expand to see a compressed algebraic derivation of the ELBO</summary> <p>Ignoring the fact that $\widetilde{p}(\theta)$ is not normalized, we can obtain the ELBO purely algebraically.</p> <p>First, let’s (improperly) write the ELBO in terms of the KL divergence between $q$ and the unnormalized $\widetilde{p}$:</p> \[\text{ELBO}(q) = -\text{KL}(q \,\mid\mid\, \widetilde{p})\] <p>Then we have four steps:</p> <ol> <li> \[\text{KL}[q \,\mid\mid\, \widetilde{p}] = \text{KL}[q || \mathcal{Z} p^\star] = \text{KL}[q \,\mid\mid\, p^\star] - \log \mathcal{Z}\] </li> <li> \[\text{KL}[q \,\mid\mid\, p^\star] = \text{KL}[q \,\mid\mid\, p] + \log \mathcal{Z}\] </li> <li> \[\text{KL}[q \,\mid\mid\, p^\star] = \log \mathcal{Z} - \text{ELBO}(q)\] </li> <li> \[\text{ELBO}(q) = \log \mathcal{Z} - \text{KL}[q \,\mid\mid\, p^\star] &lt;= \log \mathcal{Z}\] </li> </ol> <p>The much longer “full derivation” in the tab above is to avoid using the KL divergence for an unnormalized pdf, which is improper; but it is the same thing.</p> <p>We can famously <a href="https://en.wikipedia.org/wiki/Evidence_lower_bound#Deriving_the_ELBO">derive the ELBO using Jensen’s inequality</a>, but it adds an unnecessary and potentially misleading “approximate” step, when we apply the inequality. I prefer the almost trivial derivation above, which shows the relationship between the ELBO and the KL divergence purely algebraically.</p> <p>(You still need Jensen’s to show that the KL divergence is non-negative; but subjectively that feels just a property of the KL instead of being the ELBO doing something shady.)</p> </details> <h2 id="variational-inference-for-bayesian-inference">Variational inference for Bayesian inference</h2> <p>While variational inference can be performed for any generic target density \(\widetilde{p}(\theta)\), the common scenario is that our target density is a <em>posterior distribution</em>:</p> \[{p^\star}(\theta) \equiv p(\theta \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \theta) \pi(\theta)}{p(\mathcal{D})}\] <p>where you should recognize on the right-hand side good old Bayes’ theorem, with \(p(\mathcal{D} \mid \theta)\) the <em>likelihood</em> and \(\pi(\theta)\) the <em>prior</em>.<d-footnote>We denote the prior with $\pi$ to avoid confusion with the target.</d-footnote> The normalization constant at the denominator is $\mathcal{Z} \equiv p(\mathcal{D})$, also called the <em>model evidence</em> or <em>marginal likelihood</em>. See Lotfi et al. (2022)<d-cite key="lotfi2022bayesian"></d-cite> for a recent discussion of the marginal likelihood in machine learning.</p> <p>In essentially all practical scenarios we never know the normalization constant, but we can instead compute the <em>unnormalized</em> posterior:</p> \[\widetilde{p}(\theta) = p(\mathcal{D} \mid \theta) \pi(\theta).\] <p>In this typical usage-case scenario for variational inference, the ELBO reads:</p> \[\text{ELBO}(q_\psi) = \mathbb{E}_{q_\psi(\theta)}\left[ \log p(\mathcal{D} \mid \theta) \pi(\theta) \right] - \mathbb{E}_{q_\psi(\theta)}\left[\log q_\psi(\theta)\right]\] <p>where we simply replaced \(\widetilde{p}\) with the unnormalized posterior, and we switched here to the expectation notation, instead of integrals, just to show you how that would look like.</p> <h2 id="variational-inference-is-just-optimization">Variational inference is just optimization</h2> <p>In conclusion, variational inference reduces Bayesian inference to an optimization problem. You have a candidate solution $q$, and you shake it and twist it and spread it around until you maximize the ELBO. Variational inference per se is nothing more than this.<d-footnote>There are also variational inference methods that use other divergences than the reverse KL divergence, but we lose the meaning of the ELBO as a lower bound to the log normalization constant.</d-footnote></p> <p>You may have seen other introductions to or formulations of variational inference that may seem way more complicated. The point is that most variational inference <em>algorithms</em> focus on:</p> <ul> <li>Specific families of $q$ (e.g., factorized, exponential families, etc.)</li> <li>Specific ways of calculating and optimizing the ELBO (block-wise coordinate updates, stochastic gradient ascent, etc.)</li> </ul> <p>But don’t get confused: these are all <em>implementation details</em>. To reiterate, in principle you can just compute the expectation in the ELBO however you want and however it is convenient for you (e.g., by numerical integration, as we will do below), and move things around such that you maximize the ELBO. There is nothing more to it.</p> <p>Of course, there are many clever things that can be done in various special cases (including exploiting <a href="https://en.wikipedia.org/wiki/Calculus_of_variations">variational calculus</a>, hence the name), but none of those are necessary to understand variational inference. See Blei et al. (2017)<d-cite key="blei2017variational"></d-cite> for a review of various approaches.</p> <h3 id="variational-inference-is-just-inference">Variational inference is just inference</h3> <p>For the reasons mentioned above, I believe that variational inference is possibly the most natural way of thinking about Bayesian inference: computing the posterior is not some esoteric procedure, but we are just trying to find the distribution that best matches the true target posterior, which we know up to a constant.</p> <p>Variational inference is often seen as “just an approximation method” – as opposed to a true technique for performing Bayesian inference – because historically we were forced to use very simple approximation families (factorized, simple Gaussians, etc.). However, it has been a while since we can use very flexible distributions, starting for example from the advent of normalizing flows in the 2010s. See the poignant review paper by Papamakarios et al. (2021).<d-cite key="papamakarios2021normalizing"></d-cite></p> <p>But even old-school distributions such as mixtures of Gaussians can go a long way, as long as you use enough components; the difficulty there is to fit them accurately. <em>For example</em>,<d-footnote>Self-promotion alert!</d-footnote> our sample-efficient <a href="https://acerbilab.github.io/pyvbmc/">Variational Bayesian Monte Carlo method</a> uses a mixture of Gaussians with <em>many</em> components (up to 50) to achieve pretty-good approximations of the target. We can do that reliably and efficiently because we exploit a Gaussian process approximation of the log target which lets us calculate the cross-entropy term of the ELBO <em>analytically</em>, thus yielding very accurate gradients. I will probably write a separate post on this, and more details in Acerbi (2018, 2020).<d-cite key="acerbi2018variational"></d-cite><d-cite key="acerbi2020variational"></d-cite></p> <h2 id="playing-with-variational-inference">Playing with variational inference</h2> <p>In the widget below (<a href="https://lacerbi.github.io/interactive-vi-demo/">app page</a>) you can see variational inference at work for yourself. This works best on a computer, some aspects are not ideal on mobile.</p> <p>You can select the target density as well as the family of variational posterior, from a single Gaussian with different constraints (isotropic, diagonal covariance, full covariance) to various mixtures of Gaussians.</p> <p>Your job: click and drag around the distributions and change their parameters – or just lazily press <em>Optimize</em> – and see the ELBO value go up, getting closer and closer to the true \(\log \mathcal{Z}\), as far as the chosen posterior family allows.</p> <p>It is very satisfying.</p> <iframe src="https://lacerbi.github.io/interactive-vi-demo/" width="100%" height="740px" style="border: none; margin-bottom: 40px;" title="Interactive Variational Inference Demo"> </iframe> <p>In the widget above, the ELBO is calculated via numerical integration on a grid centered around each Gaussian component, and the gradient used for optimization is calculated via <a href="https://en.wikipedia.org/wiki/Finite_difference">finite differences</a>. That’s it, nothing fancy.</p> <p>Incidentally, I spent way too much time coding up this widget, even with the help of Claude. Still, I am pretty happy with the result given that I knew zero JavaScript when I started; and I would not have done it if I had to learn JavaScript just for this. I will probably write a blog post about the process at some point.</p> <blockquote> <p>I will be hiring postdocs in early 2025 to work on extending Variational Bayesian Monte Carlo<d-cite key="acerbi2018variational"></d-cite><d-cite key="acerbi2020variational"></d-cite> and related topics. If interested, please get in touch – we can also meet at NeurIPS in Vancouver!</p> </blockquote>]]></content><author><name>Luigi Acerbi</name></author><category term="variational-inference"/><category term="demos"/><summary type="html"><![CDATA[my take on variational inference with an interactive demo]]></summary></entry><entry><title type="html">Hello world</title><link href="https://lacerbi.github.io/blog/2024/hello-world/" rel="alternate" type="text/html" title="Hello world"/><published>2024-11-21T10:32:13+00:00</published><updated>2024-11-21T10:32:13+00:00</updated><id>https://lacerbi.github.io/blog/2024/hello-world</id><content type="html" xml:base="https://lacerbi.github.io/blog/2024/hello-world/"><![CDATA[<h2 id="hello-world">Hello World</h2> <p>This is just a test of the new website blog.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="hellos"/><category term="worlds"/><summary type="html"><![CDATA[Just a test post.]]></summary></entry></feed>