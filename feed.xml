<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://lacerbi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lacerbi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-27T17:11:40+00:00</updated><id>https://lacerbi.github.io/feed.xml</id><title type="html">blank</title><subtitle>Luigi Acerbi&apos;s personal webpage. </subtitle><entry><title type="html">Playing with variational inference</title><link href="https://lacerbi.github.io/blog/2024/playing-variational-inference/" rel="alternate" type="text/html" title="Playing with variational inference"/><published>2024-11-27T00:00:00+00:00</published><updated>2024-11-27T00:00:00+00:00</updated><id>https://lacerbi.github.io/blog/2024/playing-variational-inference</id><content type="html" xml:base="https://lacerbi.github.io/blog/2024/playing-variational-inference/"><![CDATA[<h2 id="variational-inference-on-a-general-target-density">Variational inference on a general target density</h2> <p>In general, variational inference approximates a target (unnormalized) distribution \(p_\text{target}(\theta)\) with a simpler distribution \(q_\psi(\theta)\) parameterized by \(\psi\).</p> <p>For example, if \(q\) is a multivariate normal, \(\psi\) could be the mean and covariance matrix of the distribution, \(\psi = (\mu, \Sigma)\). Please note that while normal distributions are a common choice in variational inference, they are not the only one – you could choose \(q\) to be <em>any</em> distribution of your choice!</p> <p>For a given family of approximating distributions \(q_\psi(\theta)\), variational inference chooses the best value of the parameters \(\psi\) that make \(q_\psi\) “as close as possible” to \(p\) by maximizing the ELBO (evidence lower bound):</p> \[\text{ELBO}(\psi) = \mathbb{E}_{q_\psi(\theta)}\left[ \log p_\text{target}(\theta)\right] - \mathbb{E}_{q_\psi(\theta)}\left[\log q_\psi(\theta)\right]\] <table> <tbody> <tr> <td>It can be shown that maximizing the ELBO is equivalent to minimizing $$ D_\text{KL}(q_\psi</td> <td> </td> <td>p_\text{target}) \(, which is the Kullback-Leibler divergence between\) q_\psi(\theta) \(and\) p_\text{target}(\theta) $$.</td> </tr> </tbody> </table> <h2 id="variational-inference-to-approximate-a-target-posterior">Variational inference to approximate a target posterior</h2> <p>While variational inference could be performed for any generic target density \(p_\text{target}(\theta)\), the common scenario is that our target density is an unnormalized posterior distribution:</p> \[p_\text{target}(\theta) = p(\mathcal{D} | \theta) p(\theta) \propto \frac{p(\mathcal{D} | \theta) p(\theta)}{p(\mathcal{D})}\] <table> <tbody> <tr> <td>where $$ p(\mathcal{D}</td> <td>\theta) p(\theta) = p(\mathcal{D}, \theta) $$ is the joint distribution. The (unknown) normalization constant is $p(\mathcal{D})$, also called the <em>model evidence</em> or <em>marginal likelihood</em>. In this typical usage-case scenario for variational inference, the ELBO reads</td> </tr> </tbody> </table> \[\text{ELBO}(\psi) = \mathbb{E}_{q_\psi(\theta)}\left[ \log p(\mathcal{D}|\theta) p(\theta) \right] - \mathbb{E}_{q_\psi(\theta)}\left[\log q_\psi(\theta)\right]\] <p>where we simply replaced \(p_\text{target}\) with the unnormalized posterior.</p> <h2 id="things-to-know">Things to know</h2> <ul> <li> <table> <tbody> <tr> <td>The term $$ \mathbb{E}<em>{q</em>\psi(\theta)}\left[ \log p(\theta</td> <td>\mathcal{D}) p(\theta) \right] $$ in the ELBO is the expected log joint.</td> </tr> </tbody> </table> </li> <li>The term \(-\mathbb{E}_{q_\psi(\theta)}\left[ \log q_\psi(\theta) \right]\) is the <em>entropy</em> of $q_{\psi}(\theta)$, often written as $\mathcal{H}[q]$.</li> <li>Note that the ELBO is a function of \(\psi\). The optimization finds the \(\psi^*\) that maximizes the ELBO (in practice, the value \(\psi^*\) that minimizes the negative ELBO).</li> <li>The ELBO is a lower bound to the log normalization constant of the target density, that is \(\log p(\mathcal{D})\) when the target is the unnormalized posterior.</li> <li>For notational convenience, the dependence of \(q_\psi(\theta)\) on \(\psi\) is often omitted. Also $$ \psi$ $ is an arbitrary notation, you will find other (Greek) letters to denote the variational parameters.</li> </ul>]]></content><author><name>Luigi Acerbi</name></author><category term="variational-inference"/><summary type="html"><![CDATA[an interactive tutorial on variational inference]]></summary></entry><entry><title type="html">Hello world</title><link href="https://lacerbi.github.io/blog/2024/hello-world/" rel="alternate" type="text/html" title="Hello world"/><published>2024-11-21T10:32:13+00:00</published><updated>2024-11-21T10:32:13+00:00</updated><id>https://lacerbi.github.io/blog/2024/hello-world</id><content type="html" xml:base="https://lacerbi.github.io/blog/2024/hello-world/"><![CDATA[<h2 id="hello-world">Hello World</h2> <p>Very creative, congratulations. This is just a test of the new theme. Bye.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="hellos"/><category term="worlds"/><summary type="html"><![CDATA[Just a test post. Maybe one day there will be real posts, but never trust an academic's promise.]]></summary></entry></feed>