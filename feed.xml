<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://lacerbi.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lacerbi.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-11-27T19:44:17+00:00</updated><id>https://lacerbi.github.io/feed.xml</id><title type="html">blank</title><subtitle>Luigi Acerbi&apos;s personal webpage. </subtitle><entry><title type="html">Vi Is Just Inference</title><link href="https://lacerbi.github.io/blog/2024/vi-is-just-inference/" rel="alternate" type="text/html" title="Vi Is Just Inference"/><published>2024-11-27T00:00:00+00:00</published><updated>2024-11-27T00:00:00+00:00</updated><id>https://lacerbi.github.io/blog/2024/vi-is-just-inference</id><content type="html" xml:base="https://lacerbi.github.io/blog/2024/vi-is-just-inference/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Variational inference is just Bayesian inference</title><link href="https://lacerbi.github.io/blog/2024/vi-is-just-inference/" rel="alternate" type="text/html" title="Variational inference is just Bayesian inference"/><published>2024-11-27T00:00:00+00:00</published><updated>2024-11-27T00:00:00+00:00</updated><id>https://lacerbi.github.io/blog/2024/vi-is-just-inference</id><content type="html" xml:base="https://lacerbi.github.io/blog/2024/vi-is-just-inference/"><![CDATA[<p>The goal of this post is to show you that variational inference is a very natural way of thinking about Bayesian inference and not some shady approximate method.<d-footnote>Unlike what the MCMC mafia wants you to think.</d-footnote> At the end, you will also be able to directly play around with variational inference via an interactive visualization. In fact, you can also just skip the article and go straight to play with the interactive thingie at the bottom.</p> <h2 id="what-is-variational-inference">What is variational inference</h2> <p>Let’s start with the textbook definitions. At the core, variational inference is a way to approximate a distribution, the <em>target</em> \(p_\text{target}(\theta)\). Importantly, the target can be an <em>unnormalized</em> probability distribution, with (unknown) normalization constant \(\mathcal{Z}\). The target can be continuous or discrete (or mixed), there are no restrictions!</p> <p>If we go on reading a textbook, it will tell us that variational inference “approximates” the target with a distribution \(q_\psi(\theta)\) parameterized by \(\psi\).</p> <p>For example, if \(q\) is a multivariate normal, \(\psi\) could be the mean and covariance matrix of the distribution, \(\psi = (\mu, \Sigma)\). Please note that while normal distributions are a common choice in variational inference, they are not the only one – you could choose \(q\) to be <em>any</em> distribution of your choice!</p> <h3 id="why-do-we-want-to-approximate-the-target">Why do we want to approximate the target?</h3> <p>That is a great question. The point is that we often do not really <em>know</em> the target. Yes, we may be able to evaluate \(p_\text{target}(\theta)\) for any chosen value of \(\theta\), but that alone does not tell us much. What is the shape of this distribution? What are its moments? Its covariance structure? Does it have multiple modes? Quite generally, what is the expectation of an arbitrary function \(f(\theta)\) under the target? We don’t know any of that!</p> <p><em>One way</em> to compute these values might be to get samples from the target… but how do we get those? How do we sample from the target?</p> <p>In short, we have our largely-unknown target \(p_\text{target}(\theta)\) and we would like to replace it with something that is easy to use. There is a magical imponderable word for that: we want our distribution to be <em>tractable</em>.</p> <h3 id="making-the-target-tractable">Making the target tractable</h3> <p>This is the key of what variational inference does: it takes an intractable target distribution and it gives back a <em>tractable</em> one (belonging to a class of our choice). What tractable exactly means is up for discussion, but at the very least we expect these properties:</p> <ul> <li>\(q\) is normalized</li> <li>We can draw samples from \(q\)</li> <li>We can evaluate the density of \(q\) at any point</li> </ul> <p>There is potentially a whole variety of desiderata for a tractable distribution, and you are encouraged to read Choi et al. (2020)<d-cite key="choi2020probabilistic"></d-cite>.</p> <h2 id="variational-inference-on-a-general-target-density">Variational inference on a general target density</h2> <p>In general, variational inference approximates a target (unnormalized) distribution \(p_\text{target}(\theta)\) with a simpler distribution \(q_\psi(\theta)\) parameterized by \(\psi\).</p> <p>For example, if \(q\) is a multivariate normal, \(\psi\) could be the mean and covariance matrix of the distribution, \(\psi = (\mu, \Sigma)\). Please note that while normal distributions are a common choice in variational inference, they are not the only one – you could choose \(q\) to be <em>any</em> distribution of your choice!</p> <p>For a given family of approximating distributions \(q_\psi(\theta)\), variational inference chooses the best value of the parameters \(\psi\) that make \(q_\psi\) “as close as possible” to \(p\) by maximizing the ELBO (evidence lower bound):</p> \[\text{ELBO}(\psi) = \mathbb{E}_{q_\psi(\theta)}\left[ \log p_\text{target}(\theta)\right] - \mathbb{E}_{q_\psi(\theta)}\left[\log q_\psi(\theta)\right]\] <p>It can be shown that maximizing the ELBO is equivalent to minimizing \(D_\text{KL}(q_\psi \mid\mid p_\text{target})\), which is the Kullback-Leibler divergence between \(q_\psi(\theta)\) and \(p_\text{target}(\theta)\).</p> <h2 id="variational-inference-to-approximate-a-target-posterior">Variational inference to approximate a target posterior</h2> <p>While variational inference could be performed for any generic target density \(p_\text{target}(\theta)\), the common scenario is that our target density is an unnormalized posterior distribution:</p> \[p_\text{target}(\theta) = p(\mathcal{D} \mid \theta) p(\theta) \propto \frac{p(\mathcal{D} \mid \theta) p(\theta)}{p(\mathcal{D})}\] <p>where \(p(\mathcal{D} \mid \theta) p(\theta) = p(\mathcal{D}, \theta)\) is the joint distribution. The (unknown) normalization constant is $p(\mathcal{D})$, also called the <em>model evidence</em> or <em>marginal likelihood</em>. In this typical usage-case scenario for variational inference, the ELBO reads</p> \[\text{ELBO}(\psi) = \mathbb{E}_{q_\psi(\theta)}\left[ \log p(\mathcal{D} \mid \theta) p(\theta) \right] - \mathbb{E}_{q_\psi(\theta)}\left[\log q_\psi(\theta)\right]\] <p>where we simply replaced \(p_\text{target}\) with the unnormalized posterior.</p> <h2 id="things-to-know">Things to know</h2> <ul> <li>The term \(\mathbb{E}_{q_\psi(\theta)}\left[ \log p(\theta \mid \mathcal{D}) p(\theta) \right]\) in the ELBO is the expected log joint.</li> <li>The term \(-\mathbb{E}_{q_\psi(\theta)}\left[ \log q_\psi(\theta) \right]\) is the <em>entropy</em> of $q_{\psi}(\theta)$, often written as $\mathcal{H}[q]$.</li> <li>Note that the ELBO is a function of \(\psi\). The optimization finds the \(\psi^*\) that maximizes the ELBO (in practice, the value \(\psi^*\) that minimizes the negative ELBO).</li> <li>The ELBO is a lower bound to the log normalization constant of the target density, that is \(\log p(\mathcal{D})\) when the target is the unnormalized posterior.</li> <li>For notational convenience, the dependence of \(q_\psi(\theta)\) on \(\psi\) is often omitted. Also $$ \psi$ $ is an arbitrary notation, you will find other (Greek) letters to denote the variational parameters.</li> </ul> <iframe src="https://lacerbi.github.io/interactive-vi-demo/" width="100%" height="500px" style="border: none;" title="Interactive Variational Inference Demo"> </iframe>]]></content><author><name>Luigi Acerbi</name></author><category term="variational-inference"/><summary type="html"><![CDATA[an interactive tutorial on variational inference]]></summary></entry><entry><title type="html">Hello world</title><link href="https://lacerbi.github.io/blog/2024/hello-world/" rel="alternate" type="text/html" title="Hello world"/><published>2024-11-21T10:32:13+00:00</published><updated>2024-11-21T10:32:13+00:00</updated><id>https://lacerbi.github.io/blog/2024/hello-world</id><content type="html" xml:base="https://lacerbi.github.io/blog/2024/hello-world/"><![CDATA[<h2 id="hello-world">Hello World</h2> <p>Very creative, congratulations. This is just a test of the new theme. Bye.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="hellos"/><category term="worlds"/><summary type="html"><![CDATA[Just a test post. Maybe one day there will be real posts, but never trust an academic's promise.]]></summary></entry></feed>